{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Molhiv.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mH_5c27a9kOS"
      },
      "outputs": [],
      "source": [
        "!pip install -U ogb\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu111.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cu111.html\n",
        "!pip install torch-geometric\n",
        "!pip install -U scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch_geometric\n",
        "import urllib3\n",
        "import outdated\n",
        "import torch\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from ogb.graphproppred import PygGraphPropPredDataset, Evaluator\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "!python -c \"import ogb; print(ogb.__version__)\"\n",
        "!python --version\n",
        "print(pd.__version__)\n",
        "print(np.__version__)\n",
        "print(torch.__version__)\n",
        "print(torch.version.cuda)\n",
        "print(torch_geometric.__version__)\n",
        "print(urllib3.__version__)\n",
        "print(outdated.__version__)\n",
        "\n",
        "!python3 -m pip show scikit-learn"
      ],
      "metadata": {
        "id": "rNu9Q6VF9t3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous partons du corpus ogbg-molhiv dans le but d'effectuer une classication de graphe. La tâche à effectuer est à partie d'une molécule de savoir si elle va permettre l'inhibation ou la réplication du virus du VIH."
      ],
      "metadata": {
        "id": "0PlcuklyCo3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.data import DataLoader\n",
        "import torch.optim as optim\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from ogb.graphproppred.mol_encoder import AtomEncoder\n",
        "from torch_geometric.nn import GCNConv\n",
        "from ogb.graphproppred import PygGraphPropPredDataset, Evaluator\n",
        "from tqdm import tqdm\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes_end, num_layer = 3, emb_dim=300, dropout=0.5):\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        # Nombre de couches cachées : profondeur du modèle\n",
        "        self.num_layer = num_layer\n",
        "        # dropout : permet d'éviter de tomber dans un minimum local\n",
        "        # On active pas tous les paramètres \n",
        "        self.dropout = dropout\n",
        "        # Initialise les embeddings des nodes\n",
        "        self.atom_encoder = AtomEncoder(emb_dim)\n",
        "\n",
        "        # liste des couches de convolution\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        # liste des couches de normalisation pour la convolution\n",
        "        self.batch_norms = torch.nn.ModuleList()\n",
        "\n",
        "        # on empile les couche de convolution dans la liste\n",
        "        for layer in range(num_layer):\n",
        "          # calcul de la convolution en focntions des dimensions choisies\n",
        "          self.convs.append(GCNConv(emb_dim, emb_dim))\n",
        "          # evite l'overfitting en normalisant les données : recentrage des données\n",
        "          self.batch_norms.append(torch.nn.BatchNorm1d(emb_dim))\n",
        "        \n",
        "        # liste des couches linéaires (MLP)\n",
        "        self.lins = torch.nn.ModuleList()\n",
        "        # Liste des couches de normalisation pour le MLM\n",
        "        self.bns_lins = torch.nn.ModuleList()\n",
        "\n",
        "        # on empile les couches linéaires du MLP\n",
        "        # -1 car on ne compte la couche de sortie\n",
        "        for _ in range(self.num_layer - 1):\n",
        "            self.lins.append(torch.nn.Linear(emb_dim, emb_dim))\n",
        "            self.bns_lins.append(torch.nn.BatchNorm1d(emb_dim))\n",
        "        \n",
        "        # couche de sortie pour la classification\n",
        "        self.lins.append(torch.nn.Linear(emb_dim, num_classes_end))\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "        for bn in self.batch_norms:\n",
        "            bn.reset_parameters()\n",
        "        for lin in self.lins:\n",
        "            lin.reset_parameters()\n",
        "        for bn in self.bns_lins:\n",
        "            bn.reset_parameters()\n",
        "\n",
        "    def forward(self, batched_data):\n",
        "\n",
        "        # on récupère les features node 9d, les liens entre les noeuds, batch = les noeuds\n",
        "        x, edge_index, batch = batched_data.x, batched_data.edge_index, batched_data.batch\n",
        "\n",
        "        # on récupère les embeddinds initaux des atoms\n",
        "        # on créer une liste de 1\n",
        "        h_list = [self.atom_encoder(x)]\n",
        "\n",
        "        # apprentissage des nodes embeddings\n",
        "        # on ajoute les couches de convolution\n",
        "        for i in range(self.num_layer):\n",
        "            # representer une nodes en fonction de ses voisisn (agrégation)\n",
        "            h = self.convs[i](h_list[i], edge_index)\n",
        "            h = self.batch_norms[i](h)         \n",
        "            h = F.relu(h)\n",
        "            # ajout du dropout\n",
        "            h = F.dropout(h, self.dropout, training = self.training)\n",
        "            # on ajoute le nouvel embedding dans la liste\n",
        "            h_list.append(h)\n",
        "\n",
        "        # on récupère la dernière node representation\n",
        "        #node_representation = h_list[-1]\n",
        "\n",
        "        # on concatène tous les embeddings sauvegardés\n",
        "        node_representation = 0\n",
        "        for i in range(self.num_layer + 1):\n",
        "            node_representation += h_list[i]\n",
        "    \n",
        "        # on effectue le pooling qui donne la valeurs moyenne des voisins d'une node\n",
        "        # réduit la dimension\n",
        "        # moyenne des embeddings des voisins\n",
        "        x = global_mean_pool(node_representation, batch)\n",
        "\n",
        "        # on ajoute les couches cachées du MLP\n",
        "        for i in range(self.num_layer - 1):\n",
        "           x = self.lins[i](x)\n",
        "           x = self.bns_lins[i](x) \n",
        "           # relu fonction activation : active un neurone\n",
        "           x = F.relu(x)\n",
        "           x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        # couche linéaire de sortie\n",
        "        x = self.lins[-1](x)\n",
        "        #x = self.lin(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# permet d'obtenir une classification binaire\n",
        "cls_criterion = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "def train(model, loader, optimizer):\n",
        "    model.train()\n",
        "\n",
        "    for step, batch in enumerate(tqdm(loader)):\n",
        "        batch = batch.to(device)\n",
        "        pred = model(batch)\n",
        "        optimizer.zero_grad()\n",
        "        loss = cls_criterion(pred.to(torch.float32), batch.y.to(torch.float32))\n",
        "        loss_final = loss\n",
        "        loss.backward()\n",
        "        optimizer.step()     \n",
        "    \n",
        "    return loss.item()\n",
        "\n",
        "def eval(model, loader, evaluator):\n",
        "    model.eval()\n",
        "\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for step, batch in enumerate(tqdm(loader)):\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred = model(batch)\n",
        "\n",
        "        y_true.append(batch.y.view(pred.shape).detach().cpu())\n",
        "        y_pred.append(pred.detach().cpu())\n",
        "\n",
        "    y_true = torch.cat(y_true, dim = 0).numpy()\n",
        "    y_pred = torch.cat(y_pred, dim = 0).numpy()\n",
        "\n",
        "    input_dict = {\n",
        "        \"y_true\": y_true, \n",
        "        \"y_pred\": y_pred\n",
        "        }\n",
        "\n",
        "    return evaluator.eval(input_dict)[\"rocauc\"]\n",
        "\n",
        "# Training settings\n",
        "batch_size = 64\n",
        "epochs = 50\n",
        "drop_ratio = 0.5\n",
        "emb_dim = 100\n",
        "num_layer = 4\n",
        "lr = 0.001\n",
        "runs = 1\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "dataset = PygGraphPropPredDataset(name = \"ogbg-molhiv\")\n",
        "\n",
        "split_idx = dataset.get_idx_split()\n",
        "\n",
        "evaluator = Evaluator(name = \"ogbg-molhiv\")\n",
        "\n",
        "train_loader = DataLoader(dataset[split_idx[\"train\"]], batch_size=batch_size, shuffle=True)\n",
        "valid_loader = DataLoader(dataset[split_idx[\"valid\"]], batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(dataset[split_idx[\"test\"]], batch_size=batch_size, shuffle=False)\n",
        "\n",
        "model = GCN(num_classes_end=dataset.num_tasks).to(device)\n",
        "\n",
        "for i in range(runs):\n",
        "  print(f\"[RUUUN {i}]\")\n",
        "  best_test = 0\n",
        "  best_valid = 0\n",
        "  model.reset_parameters()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "  for epoch in range(1, 1 + epochs):\n",
        "    print(f\"[Epoch {epoch}]\")\n",
        "    # train model\n",
        "    loss = train(model, train_loader, optimizer)  \n",
        "    \n",
        "    # print loss last iter\n",
        "    print(f'Loss : {loss:.2f}')    \n",
        "    print('Evaluating model with valid loader')\n",
        "    valid_acc = eval(model, valid_loader, evaluator)\n",
        "    print(f'Valid acc : {valid_acc:2f}')\n",
        "\n",
        "    print('Evaluating model with test loader : ')\n",
        "    test_acc = eval(model, test_loader, evaluator)\n",
        "    print(f'Test acc : {test_acc:2f}')\n",
        "                    \n",
        "    if test_acc > best_test:\n",
        "      best_test = test_acc\n",
        "    if valid_acc > best_valid:\n",
        "      best_valid = valid_acc\n",
        "\n",
        "  print(best_test, best_valid)\n",
        "  best_acc_valid.append(best_valid)\n",
        "  best_acc_test.append(best_test)\n",
        "\n",
        "moy_valid = sum(best_acc_valid) / len(best_acc_valid)\n",
        "moy_test = sum(best_acc_test) / len(best_acc_test)\n",
        "\n",
        "print(f'Validation {moy_valid} Test {moy_test}')"
      ],
      "metadata": {
        "id": "7nQ-U1GEk318",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0413f248-14bc-4496-e372-da493619c19b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch_geometric/deprecation.py:13: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RUUUN 0]\n",
            "[Epoch 1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 12%|█▏        | 62/515 [00:10<01:23,  5.44it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "S9rLKgPdFzRC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}